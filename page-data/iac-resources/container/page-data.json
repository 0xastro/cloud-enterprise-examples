{"componentChunkName":"component---src-pages-iac-resources-container-index-mdx","path":"/iac-resources/container/","result":{"pageContext":{"frontmatter":{"title":"IaC for Containers Registry","description":"Use IaC to create and modify the IBM Container Registry","keywords":"terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes"},"relativePagePath":"/iac-resources/container/index.mdx","titleType":"page","MdxNode":{"id":"80e366d6-308f-5976-b4ab-2b6edc01d6a4","children":[],"parent":"966f71ce-6345-5f2f-949b-a5aaca8259d4","internal":{"content":"---\ntitle: IaC for Containers Registry\ndescription: Use IaC to create and modify the IBM Container Registry\nkeywords: 'terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes'\n---\n\n<PageDescription>\n\nAutomating the management of container services on IBM Cloud including the Container Registry and Kubernetes Services (IKS)\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>IBM Cloud Container Registry</AnchorLink>\n  <AnchorLink>Docker push & pull</AnchorLink>\n  <AnchorLink>IBM Cloud Kubernetes Service</AnchorLink>\n  <AnchorLink>IKS with Terraform</AnchorLink>\n  <AnchorLink>IKS with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Deploy the Application</AnchorLink>\n  <AnchorLink>Final Code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\n## Prerequisites\n\nThe steps in this pattern require the local workstation to be configured with the IBM Cloud CLI, CLI plugins for `container-service`, `container-registry` & `schematics`, the Terraform CLI, IBM Terrraform provider and a local installation of [Docker](https://docs.docker.com/get-docker/) . For more details on setting up the various CLI environments, see the [Setup Environment](/iac/setup-environment) chapter.\n\n## IBM Cloud Container Registry\n\nIBM Cloud Container Registry (ICR) is used to store, manage and deploy private container images in a highly available and scalable architecture. You can also set up your own image namespace and push container images to them. To learn more, see the [Container Registry](https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started) documentation. There are no specific IaC steps required to enable the Container Registry, this is a capablity that is available to an IBM Cloud account without performing a service creation task.\n\nContainer images for IBM Cloud follow the [Open Container Initiative](https://www.opencontainers.org/) (OCI) standards to provide interoperability and flexibility in tooling for the container lifecycle. One well known tool for createing OCI-compliant images is `docker` which will be used for the examples in this pattern.\n\nThe `docker` command creates an image from a `Dockerfile`, which contains instructions to build the image. A `Dockerfile` might reference build artifacts in its instructions that are stored separately, such as an app, the app's configuration, and its dependencies. Images are typically stored in a registry that can either be accessible by the public (public registry) or set up with limited access for a small group of users (private registry). By using IBM Cloud Container Registry, only users with access to your IBM Cloud account through IAM can access your images.\n\nContinue using the same application from the previous patterns in order to have a simple container image that can be used with the IBM Container Registry and Kubernetes service. Create a `Dockerfile` with the following content in a directory for this project (i.e. `containers`).\n\n```Dockerfile path=Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\nCopy to the `data` folder the JSON database file `db.min.json`. Now, build and test the container image locally using docker.\n\n```bash\ndocker build -t movies .\ndocker images\n\ndocker run --name movies -d --rm -p 80:8080 -v $PWD/data:/data movies\n\ncurl http://localhost/movies/675\n\ndocker stop $(docker ps -q --filter name=movies)\n```\n\nTo create an Container Registry namespace, use the IBM Cloud CLI with the `container-registry` plugin. Make sure you have the latest version installed and you have [setup the environment](/iac/setup-environment) correctly. Namespace names (like Docker Hub and other container repositories) must be unique for a container registry region, so substitute the name shown here with a unique one of your choosing.\n\nThe sub-command `namespace-add` will create the new namespace. The examples that follow will use `iac-registry` as the namespace:\n\n```bash\nibmcloud cr namespace-list\nibmcloud cr namespace-add iac-registry\n```\n\nIn order to push your local OCI image to the namespace registry, it must be tagged as: `REGION.icr.io/NAMESPACE/IMAGE:TAG`. Use the sub-command `region` to find the registry region you are targeting:\n\n```bash\nibmcloud cr region\n```\n\nContinuing with the example, the region is `us` so the registry is `us.icr.io`. The namespace is `iac-registry`, the image name is `movies` and the version tag `1.0`. The full tag will be: `us.icr.io/iac-registry/movies:1.0`. The image has already been created with the tag `movies` so to update, use the docker `tag` command:\n\n```bash\ndocker images\ndocker tag movies us.icr.io/iac-registry/movies:1.0\n```\n\nVisit the [Docker tag](https://docs.docker.com/engine/reference/commandline/tag/) documentation to find our more about image tags.\n\nBefore pushing the image to the registry it's required to login with the `login` sub-command:\n\n```bash\nibmcloud cr login\n```\n\nThis command will set up the local `docker` cli with a credentials object that allows it to communicate to the namespaces defined for your account in the current container registry region. After logging in, push the image with the Docker command `push`:\n\n```bash\ndocker push us.icr.io/iac-registry/movies:1.0\n```\n\nYou can check the image in the registry in different ways: (1) listing the images in the registry with the `ibmcloud cr images` command, or (2) using the `docker` command to pull the image, either from a different computer or by locally deleting the image and pulling it down from the registry:\n\n```bash\nibmcloud cr images\n\ndocker rmi us.icr.io/iac-registry/movies:1.0\ndocker pull us.icr.io/iac-registry/movies:1.0\ndocker images\n```\n\nWith the container image uploaded to the IBM Container Registry, you will be able to create Kubernetes deployments of the image by specifying the path to the fully qualified tag name `us.icr.io/iac-registry/movies:1.0` . Before doing this, you will need to create an IKS cluster.\n\n## IBM Cloud Kubernetes Service\n\nIBM Cloud Kubernetes Service (IKS) is a managed offering providing dedicated Kubernetes clusters to deploy and manage containerized apps. In this section you will create a Kubernetes cluster and deploy a simple API application. Examples will be provided using IBM Cloud CLI, Terraform and Schematics. The scope of this section is to cover creation of clusters and simple application deployment using IaC techniques. It will not cover deeper details for managing Kubernetes resources in general or broadly managing Kubernetes and deployments.\n\nTo create a Kubernetes cluster using the IBM Cloud CLI you need to specify parameters such as zone and worker node flavor. Discover these using the following commands. In this example, we are using Zone `us-south-1` and worker node flavor `mx2.4x32`.\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\nFLAVOR=mx2.4x32\n```\n\nYou also need a VPC and Subnet for the Kubernetes cluster. If they do not yet exist, they may be created using the IBM Cloud CLI:\n\n```bash\n# VPC Name: iac-iks-vpc\nibmcloud is vpc-create iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"iac-iks-vpc\\\").id\")\n\n# Subnet Name iac-iks-subnet and 16 IP addresses.\nibmcloud is subnet-create iac-iks-subnet $VPC_ID --zone $ZONE --ipv4-address-count 16\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"iac-iks-subnet\\\").id\")\n```\n\nAfter the VPC is created, the default security group will not have network access rules needed by the load balancers of the Kubernetes service to talk to the ingress controllers or other applications deployed as NodePort services. Update the default security group by adding a rule.\n\n```bash\nibmcloud is security-group-create iac-iks-sg $VPC_ID    # Security Group Name: iac-iks-sg\nSG_ID=$(ibmcloud is vpc-sg $VPC_ID --json | jq -r \".id\")\n\nibmcloud is security-group-rule-add $SG_ID inbound tcp --port-min 30000 --port-max 32767\n```\n\nIf you already have a VPC and Subnets, get their IDs with the following `ibmcloud ks` sub-commands:\n\n```bash\nibmcloud ks vpcs --provider vpc-gen2        # VPC Name: iac-iks-vpc\nVPC_ID=$(ibmcloud ks vpcs --provider vpc-gen2 --json | jq -r '.[] | select(.name==\"iac-iks-vpc\").id')\n\nibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE   # Subnet Name: iac-iks-subnet\nSUBNET_ID=$(ibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE --json | jq -r '.[] | select(.name==\"iac-iks-subnet\").id')\n```\n\nThe available Kubernetes versions to install are listed with the command `ibmcloud ks versions`. For IKS on Gen2, use a kubernetes cluster version > `1.18`. With all input parameters defined, including a name and Kubernetes veyou are ready to create the cluster using the `cluster create` sub-command, like this:\n\n```bash\nNAME=iac-iks-cluster\nVERSION=1.18.2\n\nibmcloud ks cluster create vpc-gen2 \\\n  --name $NAME \\\n  --zone $ZONE \\\n  --vpc-id $VPC_ID \\\n  --subnet-id $SUBNET_ID \\\n  --flavor $FLAVOR \\\n  --version $VERSION \\\n  # --workers $N \\\n  # --entitlement cloud_pak \\\n  # --service-subnet $SUBNET_CIDR \\\n  # --pod-subnet $POD_CIDR \\\n  # --disable-public-service-endpoint \\\n```\n\nThe default values for the optional parameters are:\n\n- `N`: 1, this is a one worker node cluster.\n- `SUBNET_CIDR`: 172.21.0.0/16\n- `POD_CIDR`: 172.30.0.0/16\n- `disable-public-service-endpoint`: false\n\nTo identify your Kubernetes cluster status use the command `ibmcloud ks clusters`, wait a few minutes to have it up and running.\n\nWhen the Kubernetes cluster state is `normal` get the configuration to access the cluster using the following command:\n\n```bash\nibmcloud ks cluster config --cluster $NAME\n```\n\nNow you are ready to use the `kubectl` command, these are some initial commands:\n\n```bash\nkubectl cluster-info\nkubectl get nodes\n```\n\nYou can obtain more information of the cluster with the commands:\n\n```bash\nibmcloud ks worker ls --cluster $NAME\nibmcloud ks cluster get --cluster $NAME\n```\n\nTo know more read the [Kubernetes Service (IKS)](https://cloud.ibm.com/docs/containers?topic=containers-getting-started) documentation.\n\n## IKS with Terraform\n\nAll the same actions executed with the IBM Cloud CLI has to be done with Terraform, lets create a new `main.tf` file with the IBM Provisioner using Gen 2, the given region and the data source to get the info of the user selected resource group.\n\n```hcl path=main.tf\nprovider \"ibm\" {\n  generation = 2\n  region     = var.region\n}\n\ndata \"ibm_resource_group\" \"group\" {\n  name = var.resource_group\n}\n```\n\nThe `variables.tf` file defines the required variables above, the project name and environment to use them as prefix to name the resources, the code would be like this:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\n```\n\nTo not have to enter the variables every time we execute terraform, lets add some variables value to the `terraform.tfvars` file. Make sure this file is appended to the `.gitignore` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-test\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\n```\n\nThe IKS clusters needs a VPC, Subnet(s) and Security Group(s) just like we did using the IBM Cloud CLI. This time we'll create a VPC and multiple subnets, then one security group per subnet allowing inbound traffic to ports 30000 - 32767. Same as you did on [Network](../network/index.mdx) and [Compute](../compute/index.mdx) the number of subnets is defined by the number of zones provided by the user. Lets code this in the `network.tf` file and append the following variables to `variables.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group\" \"iac_iks_security_group\" {\n  name           = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc            = ibm_is_vpc.iac_iks_vpc.id\n  resource_group = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_security_group.iac_iks_security_group.id\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\nLast but not least, create the `iks.tf` file to define the IKS cluster using the `ibm_container_vpc_cluster` resource. The following code also takes the Kubernetes version, worker nodes flavor and number from the variables `k8s_version`, `flavor` and `workers_count`, so lets add them to the `variables.tf` file.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavor\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"flavor\" {\n  default = \"mx2.4x32\"\n}\nvariable \"workers_count\" {\n  default = 3\n}\nvariable \"k8s_version\" {\n  default = \"1.18.2\"\n}\n```\n\nThis will create a Kubernetes cluster of 3 worker nodes with 2 CPU and 4 Gb Memory. To know the available flavors in the zone, use the following IBM Cloud CLI command:\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\n\n# Or\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\n```\n\nTo sort them by CPU and memory, use the same command with `sort`:\n\n```bash\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE -s | sort -k2 -k3 -n\n```\n\nThe main input parameters of the `ibm_container_vpc_cluster` resource are listed in the following table:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the cluster |\n| `vpc_id` | ID of the VPC that you want to use for your cluster |\n| `flavor` | flavor of the VPC worker node |\n| `zones` | nested block describing the zones of this VPC cluster |\n| `zones.name` | name of the zone |\n| `zones.subnet_id` | subnet in the zone to assign the cluster |\n| `worker_count` | (optional) number of worker nodes per zone in the default worker pool. Default value `1` |\n| `kube_version` | (optional) Kubernetes version, including the major.minor version. If not set, the default version from `ibmcloud ks versions` is used |\n| `resource_group_id` | (optional) ID of the resource group. Defaults to `default` |\n| `wait_till` | (optional) marks the creation of your cluster complete when the given stage is achieved, read below to know the available stages and how this can help you speed up the terraform execution |\n| `disable_public_service_endpoint` | (optional) disable the master public service endpoint to prevent public access. Defaults to `true` |\n| `pod_subnet` | (optional) subnet CIDR to provide private IP addresses for pods. Defaults to `172.30.0.0/16` |\n| `service_subnet` | (optional) subnet CIDR to provide private IP addresses for services. Defaults to `172.21.0.0/16` |\n| `tags` | (optional) list of tags to associate with your cluster |\n\nThe creation of a cluster can take some minutes to complete. To avoid long wait times, you can specify the stage when you want Terraform to mark the cluster resource creation as completed. The cluster creation might not be fully completed and continues to run in the background, however this can help you to continue with the code execution without waiting for the cluster to be fully created.\n\nTo set the waiting stage, use the `wait_till` with one of the following stages:\n\n- **MasterNodeReady**: Terraform marks the creation of your cluster complete when the cluster master is in a ready state.\n- **OneWorkerNodeReady**: Waits until the master and at least one worker node are in a ready state.\n- **IngressReady**: Waits until the cluster master and all worker nodes are in a ready state, and the Ingress subdomain is fully set up. This is the default value.\n\nThis would be enough to have an IKS cluster running. Just need to execute `terraform apply`, however lets create workers pools, one in each subnet or zone, using the resource `ibm_container_vpc_worker_pool`. Replace the code in `iks.tf` file for the following code and modify the variables used for the number of workers and its flavor.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\n```\n\nThe main input parameters for the `ibm_container_vpc_worker_pool` resource are similar to the parameters for `ibm_container_vpc_cluster` except for `worker_pool_name` to name the pool and `cluster` with the name or ID of the cluster set this pool.\n\nUsing a file `output.tf` helps us to get some useful information about the cluster through output variables, like so.\n\n```hcl path=output.tf\n\n```\n\nNow everything is ready to create the cluster with the wellknown Terraform commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nAfter having the cluster ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl`, like so:\n\n```bash\nibmcloud ks cluster config --cluster $(terraform output cluster_id)\n```\n\nEnjoy the new cluster, here are some basic initial commands to verify the cluster is working\n\n```bash\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n### A simpler IKS cluster\n\nFor simplicity and creation speed, lets modify the `terraform.tfvars` to have a simpler cluster with one single node. This will help us to have the cluster quicker.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-small-OWNER\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\nvpc_zone_names = [\"us-south-1\"]\nflavors        = [\"mx2.4x32\"]\nworkers_count  = [1]\nk8s_version    = \"1.18.2\"\n```\n\nExecuting `terraform plan & terraform apply` will get an IKS cluster up and running quicker than before.\n\n## IKS with IBM Cloud Schematics\n\nRunning this code with IBM Cloud Schematics is the same as with the other patterns. Create the `workspace.json` file adding the variables required for this code, like this one:\n\n```json path=workspace.json\n{\n  \"name\": \"iac_iks_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:iac_iks_test\",\n    \"owner:OWNER\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac-iks-test-OWNER\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"region\",\n        \"value\": \"us-south\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"vpc_zone_names\",\n        \"value\": [\"us-south-1\", \"us-south-2\", \"us-south-3\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"flavors\",\n        \"value\": [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"workers_count\",\n        \"value\": [3, 2, 1],\n        \"type\": \"list(number)\"\n      },\n      {\n        \"name\": \"k8s_version\",\n        \"value\": \"1.18.2\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nTo create the workspace using the IBM Cloud CLI execute the following commands:\n\n```bash\nibmcloud schematics workspace new --file workspace.json\nibmcloud schematics workspace list          # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n```\n\nSet the variable `` because it'll be used several times. Then plan and apply the code like so.\n\n```bash\nibmcloud schematics plan --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\nibmcloud schematics apply --id $WORKSPACE_ID # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n```\n\nNote the execution of apply will take some time, so check the logs either with the IBM Cloud CLI command or using the IBM Cloud Web Console.\n\n## Deploy the Application\n\nTo deploy the previously built Docker image we use the Kubernetes API and resources. Lets create a deployment file either by getting it from the following example or generating it with kubectl generators, like so:\n\n```bash\nmkdir kubernetes\nkubectl create deployment movies --image=us.icr.io/iac-registry/movies:1.0 --dry-run=client -o yaml > kubernetes/deployment.yaml\nkubectl create service nodeport movies --tcp=8080 --node-port=32000 --dry-run=client -o yaml > kubernetes/service.yaml\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"80\"\n      nodePort: 32000\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: movies\n  type: NodePort\nstatus:\n  loadBalancer: {}\n```\n\nTo deploy the application execute the `kubectl apply` command like this:\n\n```bash\nkubectl apply -f kubernetes\n```\n\n## Final Code\n\nAll the code used for this pattern is located and available to download in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [09-containers](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers) where the main files are:\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group\" \"iac_iks_security_group\" {\n  name           = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc            = ibm_is_vpc.iac_iks_vpc.id\n  resource_group = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_security_group.iac_iks_security_group.id\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\nvariable \"k8s_version\" {\n  default = \"1.18.2\"\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\n```Dockerfile path=Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"80\"\n      nodePort: 32000\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: movies\n  type: NodePort\nstatus:\n  loadBalancer: {}\n```\n\n## Clean up\n\nWhen you are done with the Kubernetes cluster should destroy it.\n\nIf the cluster was created **using the IBM Cloud CLI**, execute the following commands:\n\n```bash\nNAME=iac-iks-cluster\nibmcloud ks cluster rm --cluster $NAME\n\nSG_Name=iac-iks-sg\nSG_ID=$(ibmcloud is security-groups --json | jq -r \".[] | select(.name==\\\"$SG_Name\\\").id\")\nibmcloud is security-group-delete $SG_ID\n\nSubnet_Name=iac-iks-subnet\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"$Subnet_Name\\\").id\")\nibmcloud is subnet-delete $SUBNET_ID\n\nVPC_Name=iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"$VPC_Name\\\").id\")\nibmcloud is vpc-delete $VPC_Name\n```\n\nIf the cluster was created **using Terraform**, just need to execute the command:\n\n```bash\nterraform destroy\n```\n\nAnd, if the cluster was created **using IBM Cloud Schematics**, execute the following commands:\n\n```bash\nibmcloud schematics workspace list              # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n\nibmcloud schematics destroy --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\n# ... wait until it's done\n\nibmcloud schematics workspace delete --id $WORKSPACE_ID\nibmcloud schematics workspace list\n```\n","type":"Mdx","contentDigest":"7c4293b0a6831ca2c7b4e96d465b7d83","counter":692,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IaC for Containers Registry","description":"Use IaC to create and modify the IBM Container Registry","keywords":"terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes"},"exports":{},"rawBody":"---\ntitle: IaC for Containers Registry\ndescription: Use IaC to create and modify the IBM Container Registry\nkeywords: 'terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes'\n---\n\n<PageDescription>\n\nAutomating the management of container services on IBM Cloud including the Container Registry and Kubernetes Services (IKS)\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>IBM Cloud Container Registry</AnchorLink>\n  <AnchorLink>Docker push & pull</AnchorLink>\n  <AnchorLink>IBM Cloud Kubernetes Service</AnchorLink>\n  <AnchorLink>IKS with Terraform</AnchorLink>\n  <AnchorLink>IKS with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Deploy the Application</AnchorLink>\n  <AnchorLink>Final Code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\n## Prerequisites\n\nThe steps in this pattern require the local workstation to be configured with the IBM Cloud CLI, CLI plugins for `container-service`, `container-registry` & `schematics`, the Terraform CLI, IBM Terrraform provider and a local installation of [Docker](https://docs.docker.com/get-docker/) . For more details on setting up the various CLI environments, see the [Setup Environment](/iac/setup-environment) chapter.\n\n## IBM Cloud Container Registry\n\nIBM Cloud Container Registry (ICR) is used to store, manage and deploy private container images in a highly available and scalable architecture. You can also set up your own image namespace and push container images to them. To learn more, see the [Container Registry](https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started) documentation. There are no specific IaC steps required to enable the Container Registry, this is a capablity that is available to an IBM Cloud account without performing a service creation task.\n\nContainer images for IBM Cloud follow the [Open Container Initiative](https://www.opencontainers.org/) (OCI) standards to provide interoperability and flexibility in tooling for the container lifecycle. One well known tool for createing OCI-compliant images is `docker` which will be used for the examples in this pattern.\n\nThe `docker` command creates an image from a `Dockerfile`, which contains instructions to build the image. A `Dockerfile` might reference build artifacts in its instructions that are stored separately, such as an app, the app's configuration, and its dependencies. Images are typically stored in a registry that can either be accessible by the public (public registry) or set up with limited access for a small group of users (private registry). By using IBM Cloud Container Registry, only users with access to your IBM Cloud account through IAM can access your images.\n\nContinue using the same application from the previous patterns in order to have a simple container image that can be used with the IBM Container Registry and Kubernetes service. Create a `Dockerfile` with the following content in a directory for this project (i.e. `containers`).\n\n```Dockerfile path=Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\nCopy to the `data` folder the JSON database file `db.min.json`. Now, build and test the container image locally using docker.\n\n```bash\ndocker build -t movies .\ndocker images\n\ndocker run --name movies -d --rm -p 80:8080 -v $PWD/data:/data movies\n\ncurl http://localhost/movies/675\n\ndocker stop $(docker ps -q --filter name=movies)\n```\n\nTo create an Container Registry namespace, use the IBM Cloud CLI with the `container-registry` plugin. Make sure you have the latest version installed and you have [setup the environment](/iac/setup-environment) correctly. Namespace names (like Docker Hub and other container repositories) must be unique for a container registry region, so substitute the name shown here with a unique one of your choosing.\n\nThe sub-command `namespace-add` will create the new namespace. The examples that follow will use `iac-registry` as the namespace:\n\n```bash\nibmcloud cr namespace-list\nibmcloud cr namespace-add iac-registry\n```\n\nIn order to push your local OCI image to the namespace registry, it must be tagged as: `REGION.icr.io/NAMESPACE/IMAGE:TAG`. Use the sub-command `region` to find the registry region you are targeting:\n\n```bash\nibmcloud cr region\n```\n\nContinuing with the example, the region is `us` so the registry is `us.icr.io`. The namespace is `iac-registry`, the image name is `movies` and the version tag `1.0`. The full tag will be: `us.icr.io/iac-registry/movies:1.0`. The image has already been created with the tag `movies` so to update, use the docker `tag` command:\n\n```bash\ndocker images\ndocker tag movies us.icr.io/iac-registry/movies:1.0\n```\n\nVisit the [Docker tag](https://docs.docker.com/engine/reference/commandline/tag/) documentation to find our more about image tags.\n\nBefore pushing the image to the registry it's required to login with the `login` sub-command:\n\n```bash\nibmcloud cr login\n```\n\nThis command will set up the local `docker` cli with a credentials object that allows it to communicate to the namespaces defined for your account in the current container registry region. After logging in, push the image with the Docker command `push`:\n\n```bash\ndocker push us.icr.io/iac-registry/movies:1.0\n```\n\nYou can check the image in the registry in different ways: (1) listing the images in the registry with the `ibmcloud cr images` command, or (2) using the `docker` command to pull the image, either from a different computer or by locally deleting the image and pulling it down from the registry:\n\n```bash\nibmcloud cr images\n\ndocker rmi us.icr.io/iac-registry/movies:1.0\ndocker pull us.icr.io/iac-registry/movies:1.0\ndocker images\n```\n\nWith the container image uploaded to the IBM Container Registry, you will be able to create Kubernetes deployments of the image by specifying the path to the fully qualified tag name `us.icr.io/iac-registry/movies:1.0` . Before doing this, you will need to create an IKS cluster.\n\n## IBM Cloud Kubernetes Service\n\nIBM Cloud Kubernetes Service (IKS) is a managed offering providing dedicated Kubernetes clusters to deploy and manage containerized apps. In this section you will create a Kubernetes cluster and deploy a simple API application. Examples will be provided using IBM Cloud CLI, Terraform and Schematics. The scope of this section is to cover creation of clusters and simple application deployment using IaC techniques. It will not cover deeper details for managing Kubernetes resources in general or broadly managing Kubernetes and deployments.\n\nTo create a Kubernetes cluster using the IBM Cloud CLI you need to specify parameters such as zone and worker node flavor. Discover these using the following commands. In this example, we are using Zone `us-south-1` and worker node flavor `mx2.4x32`.\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\nFLAVOR=mx2.4x32\n```\n\nYou also need a VPC and Subnet for the Kubernetes cluster. If they do not yet exist, they may be created using the IBM Cloud CLI:\n\n```bash\n# VPC Name: iac-iks-vpc\nibmcloud is vpc-create iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"iac-iks-vpc\\\").id\")\n\n# Subnet Name iac-iks-subnet and 16 IP addresses.\nibmcloud is subnet-create iac-iks-subnet $VPC_ID --zone $ZONE --ipv4-address-count 16\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"iac-iks-subnet\\\").id\")\n```\n\nAfter the VPC is created, the default security group will not have network access rules needed by the load balancers of the Kubernetes service to talk to the ingress controllers or other applications deployed as NodePort services. Update the default security group by adding a rule.\n\n```bash\nibmcloud is security-group-create iac-iks-sg $VPC_ID    # Security Group Name: iac-iks-sg\nSG_ID=$(ibmcloud is vpc-sg $VPC_ID --json | jq -r \".id\")\n\nibmcloud is security-group-rule-add $SG_ID inbound tcp --port-min 30000 --port-max 32767\n```\n\nIf you already have a VPC and Subnets, get their IDs with the following `ibmcloud ks` sub-commands:\n\n```bash\nibmcloud ks vpcs --provider vpc-gen2        # VPC Name: iac-iks-vpc\nVPC_ID=$(ibmcloud ks vpcs --provider vpc-gen2 --json | jq -r '.[] | select(.name==\"iac-iks-vpc\").id')\n\nibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE   # Subnet Name: iac-iks-subnet\nSUBNET_ID=$(ibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE --json | jq -r '.[] | select(.name==\"iac-iks-subnet\").id')\n```\n\nThe available Kubernetes versions to install are listed with the command `ibmcloud ks versions`. For IKS on Gen2, use a kubernetes cluster version > `1.18`. With all input parameters defined, including a name and Kubernetes veyou are ready to create the cluster using the `cluster create` sub-command, like this:\n\n```bash\nNAME=iac-iks-cluster\nVERSION=1.18.2\n\nibmcloud ks cluster create vpc-gen2 \\\n  --name $NAME \\\n  --zone $ZONE \\\n  --vpc-id $VPC_ID \\\n  --subnet-id $SUBNET_ID \\\n  --flavor $FLAVOR \\\n  --version $VERSION \\\n  # --workers $N \\\n  # --entitlement cloud_pak \\\n  # --service-subnet $SUBNET_CIDR \\\n  # --pod-subnet $POD_CIDR \\\n  # --disable-public-service-endpoint \\\n```\n\nThe default values for the optional parameters are:\n\n- `N`: 1, this is a one worker node cluster.\n- `SUBNET_CIDR`: 172.21.0.0/16\n- `POD_CIDR`: 172.30.0.0/16\n- `disable-public-service-endpoint`: false\n\nTo identify your Kubernetes cluster status use the command `ibmcloud ks clusters`, wait a few minutes to have it up and running.\n\nWhen the Kubernetes cluster state is `normal` get the configuration to access the cluster using the following command:\n\n```bash\nibmcloud ks cluster config --cluster $NAME\n```\n\nNow you are ready to use the `kubectl` command, these are some initial commands:\n\n```bash\nkubectl cluster-info\nkubectl get nodes\n```\n\nYou can obtain more information of the cluster with the commands:\n\n```bash\nibmcloud ks worker ls --cluster $NAME\nibmcloud ks cluster get --cluster $NAME\n```\n\nTo know more read the [Kubernetes Service (IKS)](https://cloud.ibm.com/docs/containers?topic=containers-getting-started) documentation.\n\n## IKS with Terraform\n\nAll the same actions executed with the IBM Cloud CLI has to be done with Terraform, lets create a new `main.tf` file with the IBM Provisioner using Gen 2, the given region and the data source to get the info of the user selected resource group.\n\n```hcl path=main.tf\nprovider \"ibm\" {\n  generation = 2\n  region     = var.region\n}\n\ndata \"ibm_resource_group\" \"group\" {\n  name = var.resource_group\n}\n```\n\nThe `variables.tf` file defines the required variables above, the project name and environment to use them as prefix to name the resources, the code would be like this:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\n```\n\nTo not have to enter the variables every time we execute terraform, lets add some variables value to the `terraform.tfvars` file. Make sure this file is appended to the `.gitignore` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-test\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\n```\n\nThe IKS clusters needs a VPC, Subnet(s) and Security Group(s) just like we did using the IBM Cloud CLI. This time we'll create a VPC and multiple subnets, then one security group per subnet allowing inbound traffic to ports 30000 - 32767. Same as you did on [Network](../network/index.mdx) and [Compute](../compute/index.mdx) the number of subnets is defined by the number of zones provided by the user. Lets code this in the `network.tf` file and append the following variables to `variables.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group\" \"iac_iks_security_group\" {\n  name           = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc            = ibm_is_vpc.iac_iks_vpc.id\n  resource_group = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_security_group.iac_iks_security_group.id\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\nLast but not least, create the `iks.tf` file to define the IKS cluster using the `ibm_container_vpc_cluster` resource. The following code also takes the Kubernetes version, worker nodes flavor and number from the variables `k8s_version`, `flavor` and `workers_count`, so lets add them to the `variables.tf` file.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavor\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"flavor\" {\n  default = \"mx2.4x32\"\n}\nvariable \"workers_count\" {\n  default = 3\n}\nvariable \"k8s_version\" {\n  default = \"1.18.2\"\n}\n```\n\nThis will create a Kubernetes cluster of 3 worker nodes with 2 CPU and 4 Gb Memory. To know the available flavors in the zone, use the following IBM Cloud CLI command:\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\n\n# Or\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\n```\n\nTo sort them by CPU and memory, use the same command with `sort`:\n\n```bash\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE -s | sort -k2 -k3 -n\n```\n\nThe main input parameters of the `ibm_container_vpc_cluster` resource are listed in the following table:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the cluster |\n| `vpc_id` | ID of the VPC that you want to use for your cluster |\n| `flavor` | flavor of the VPC worker node |\n| `zones` | nested block describing the zones of this VPC cluster |\n| `zones.name` | name of the zone |\n| `zones.subnet_id` | subnet in the zone to assign the cluster |\n| `worker_count` | (optional) number of worker nodes per zone in the default worker pool. Default value `1` |\n| `kube_version` | (optional) Kubernetes version, including the major.minor version. If not set, the default version from `ibmcloud ks versions` is used |\n| `resource_group_id` | (optional) ID of the resource group. Defaults to `default` |\n| `wait_till` | (optional) marks the creation of your cluster complete when the given stage is achieved, read below to know the available stages and how this can help you speed up the terraform execution |\n| `disable_public_service_endpoint` | (optional) disable the master public service endpoint to prevent public access. Defaults to `true` |\n| `pod_subnet` | (optional) subnet CIDR to provide private IP addresses for pods. Defaults to `172.30.0.0/16` |\n| `service_subnet` | (optional) subnet CIDR to provide private IP addresses for services. Defaults to `172.21.0.0/16` |\n| `tags` | (optional) list of tags to associate with your cluster |\n\nThe creation of a cluster can take some minutes to complete. To avoid long wait times, you can specify the stage when you want Terraform to mark the cluster resource creation as completed. The cluster creation might not be fully completed and continues to run in the background, however this can help you to continue with the code execution without waiting for the cluster to be fully created.\n\nTo set the waiting stage, use the `wait_till` with one of the following stages:\n\n- **MasterNodeReady**: Terraform marks the creation of your cluster complete when the cluster master is in a ready state.\n- **OneWorkerNodeReady**: Waits until the master and at least one worker node are in a ready state.\n- **IngressReady**: Waits until the cluster master and all worker nodes are in a ready state, and the Ingress subdomain is fully set up. This is the default value.\n\nThis would be enough to have an IKS cluster running. Just need to execute `terraform apply`, however lets create workers pools, one in each subnet or zone, using the resource `ibm_container_vpc_worker_pool`. Replace the code in `iks.tf` file for the following code and modify the variables used for the number of workers and its flavor.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\n```\n\nThe main input parameters for the `ibm_container_vpc_worker_pool` resource are similar to the parameters for `ibm_container_vpc_cluster` except for `worker_pool_name` to name the pool and `cluster` with the name or ID of the cluster set this pool.\n\nUsing a file `output.tf` helps us to get some useful information about the cluster through output variables, like so.\n\n```hcl path=output.tf\n\n```\n\nNow everything is ready to create the cluster with the wellknown Terraform commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nAfter having the cluster ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl`, like so:\n\n```bash\nibmcloud ks cluster config --cluster $(terraform output cluster_id)\n```\n\nEnjoy the new cluster, here are some basic initial commands to verify the cluster is working\n\n```bash\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n### A simpler IKS cluster\n\nFor simplicity and creation speed, lets modify the `terraform.tfvars` to have a simpler cluster with one single node. This will help us to have the cluster quicker.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-small-OWNER\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\nvpc_zone_names = [\"us-south-1\"]\nflavors        = [\"mx2.4x32\"]\nworkers_count  = [1]\nk8s_version    = \"1.18.2\"\n```\n\nExecuting `terraform plan & terraform apply` will get an IKS cluster up and running quicker than before.\n\n## IKS with IBM Cloud Schematics\n\nRunning this code with IBM Cloud Schematics is the same as with the other patterns. Create the `workspace.json` file adding the variables required for this code, like this one:\n\n```json path=workspace.json\n{\n  \"name\": \"iac_iks_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:iac_iks_test\",\n    \"owner:OWNER\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac-iks-test-OWNER\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"region\",\n        \"value\": \"us-south\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"vpc_zone_names\",\n        \"value\": [\"us-south-1\", \"us-south-2\", \"us-south-3\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"flavors\",\n        \"value\": [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"workers_count\",\n        \"value\": [3, 2, 1],\n        \"type\": \"list(number)\"\n      },\n      {\n        \"name\": \"k8s_version\",\n        \"value\": \"1.18.2\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nTo create the workspace using the IBM Cloud CLI execute the following commands:\n\n```bash\nibmcloud schematics workspace new --file workspace.json\nibmcloud schematics workspace list          # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n```\n\nSet the variable `` because it'll be used several times. Then plan and apply the code like so.\n\n```bash\nibmcloud schematics plan --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\nibmcloud schematics apply --id $WORKSPACE_ID # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n```\n\nNote the execution of apply will take some time, so check the logs either with the IBM Cloud CLI command or using the IBM Cloud Web Console.\n\n## Deploy the Application\n\nTo deploy the previously built Docker image we use the Kubernetes API and resources. Lets create a deployment file either by getting it from the following example or generating it with kubectl generators, like so:\n\n```bash\nmkdir kubernetes\nkubectl create deployment movies --image=us.icr.io/iac-registry/movies:1.0 --dry-run=client -o yaml > kubernetes/deployment.yaml\nkubectl create service nodeport movies --tcp=8080 --node-port=32000 --dry-run=client -o yaml > kubernetes/service.yaml\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"80\"\n      nodePort: 32000\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: movies\n  type: NodePort\nstatus:\n  loadBalancer: {}\n```\n\nTo deploy the application execute the `kubectl apply` command like this:\n\n```bash\nkubectl apply -f kubernetes\n```\n\n## Final Code\n\nAll the code used for this pattern is located and available to download in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [09-containers](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers) where the main files are:\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group\" \"iac_iks_security_group\" {\n  name           = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc            = ibm_is_vpc.iac_iks_vpc.id\n  resource_group = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_security_group.iac_iks_security_group.id\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\nvariable \"k8s_version\" {\n  default = \"1.18.2\"\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\n```Dockerfile path=Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"80\"\n      nodePort: 32000\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: movies\n  type: NodePort\nstatus:\n  loadBalancer: {}\n```\n\n## Clean up\n\nWhen you are done with the Kubernetes cluster should destroy it.\n\nIf the cluster was created **using the IBM Cloud CLI**, execute the following commands:\n\n```bash\nNAME=iac-iks-cluster\nibmcloud ks cluster rm --cluster $NAME\n\nSG_Name=iac-iks-sg\nSG_ID=$(ibmcloud is security-groups --json | jq -r \".[] | select(.name==\\\"$SG_Name\\\").id\")\nibmcloud is security-group-delete $SG_ID\n\nSubnet_Name=iac-iks-subnet\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"$Subnet_Name\\\").id\")\nibmcloud is subnet-delete $SUBNET_ID\n\nVPC_Name=iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"$VPC_Name\\\").id\")\nibmcloud is vpc-delete $VPC_Name\n```\n\nIf the cluster was created **using Terraform**, just need to execute the command:\n\n```bash\nterraform destroy\n```\n\nAnd, if the cluster was created **using IBM Cloud Schematics**, execute the following commands:\n\n```bash\nibmcloud schematics workspace list              # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n\nibmcloud schematics destroy --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\n# ... wait until it's done\n\nibmcloud schematics workspace delete --id $WORKSPACE_ID\nibmcloud schematics workspace list\n```\n","fileAbsolutePath":"/Users/johandry/Workspace/ibm/att-cloudnative/ibmcloud-pattern-guide/src/pages/iac-resources/container/index.mdx"}}}}